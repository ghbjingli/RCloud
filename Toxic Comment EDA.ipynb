{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Machine Learning Engineer Nanodegree\n### Capstone Project\n### Project: Toxic Comment Classification with CNN\n\n### Exploratory Data Analysis\n\nIn this step, the dataset provided by Kaggle will be analyzed. The analysis will include the type of the data, basic statistics, abnormalities, etc. Where possible, visualization of the features about the data will be provided.\n\nFile descriptions\n\n    - train.csv - the training set, contains comments with their binary labels\n    - test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n    - sample_submission.csv - a sample submission file in the correct format (will not be used for capstone project)\n    - test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# import required labraries\nimport pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from os import chdir, getcwd\n\nchdir('/opt/data/share01/jl2408/')\ngetcwd() "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Read in train and test datasets\ntrain = pd.read_csv('train.csv')\ntest_cm = pd.read_csv('test.csv')\ntest_lb = pd.read_csv('test_labels.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Merge test comments with test labels\ntest_all = pd.merge(test_cm, test_lb, on='id')\n#test_all = test_all.reset_index(drop=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# use only a subset of test data since value of -1 indicates it was not used for scoring\ntest = test_all[test_all['toxic'] != -1]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# list label names\nlabel_names = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Example data in train\ntrain.loc[train[label_names].sum(axis=1) != 0].head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Example data in test\ntest_all.loc[test_all[label_names].sum(axis=1) > 0].head()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Data information in train\ntrain.info()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Data information in all test data\ntest_all.info()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Data information in test data excluding labels with -1 value\ntest.info()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# unique values in integer columns in train\npd.unique(train[label_names].values.ravel())"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# unique values in integer columns in test\npd.unique(test[label_names].values.ravel())"]},{"cell_type":"markdown","metadata":{},"source":["In summary, dataset train has 159571 comments. dataset test_all has 153164 comments. However, only 63978 comments in dataset test are used for scoring. Dataset train has 8 columns. These are id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate. Id column is just a unique identifier for each entry. comment_text will be classified. Each of the 6 categories is labeled either 0 or 1. dataset from test.csv has 2 columns. These are id and comment_text. The test_lb has 7 columns. These are id, toxic, severe_toxic, obscene, threat, insult, identity_hate. The test_labels was published after competition ended. So, we will use train dataset to train our model and use test dataset (excluding labels with -1 values) to test and score our trained model."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Check for missing values in train\nprint(train.isnull().sum())"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Check for missing values in test\nprint(test.isnull().sum())"]},{"cell_type":"markdown","metadata":{},"source":["So, there are no missing values."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Mean values for each label\ntrain[label_names].mean(axis=0)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Occurrences of each label relative to the number of samples\ntrain_lb = train[label_names]\ntrain_lb_counts = (train_lb.sum()/len(train_lb)).sort_values(ascending=False)\nprint(train_lb_counts)"]},{"cell_type":"markdown","metadata":{},"source":["Here we see about 9.6% comments in train are 'tpxic'; about 5% ar 'obscene' or 'insult'; about 1% are 'severe_toxic' or 'identity_hate'; about 0.3% is 'threat'."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Plot relative count for each toxicity category\n\nfrom collections import OrderedDict\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'brown', 5: 'black', 6: 'grey'})\ntoxicity_mapping = OrderedDict({1: 'toxic', 4: 'severe_toxic', 2: 'obscene', 6: 'threat', 3: 'insult', 5: 'identity_hate'})\ntrain_lb_counts.plot.bar(figsize = (8, 6), color = colors.values(), edgecolor = 'k', linewidth = 2)\n\n# Formatting\nplt.xlabel('Toxicity Level'); \nplt.ylabel('Count Percentage'); \nplt.xticks([x - 1 for x in toxicity_mapping.keys()], list(toxicity_mapping.values()), rotation = 60)\nplt.title('Fig. 1 Toxicity Level Breakdown');"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Plot count for multi-label coments\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n#sns.set(style='darkgrid')\nplt.figure(figsize=(8,6))\nx = train[label_names].sum(axis=1).value_counts()\nax = sns.barplot(x.index, x.values, color='blue')\n\nplt.xlabel('# of positive labels')\nplt.ylabel('count')\nplt.title(\"Fig. 2 Multi-label Comment Distrubution\")\n\npatch = mpatches.Patch(color='red', label='Count Percentage on Top of Bar')\nplt.legend(handles=[patch])\n\n# Add count on the bar\nrects = ax.patches\nlabels = x.values / len(train.index)\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    label = format(label, '.4f')\n    ax.text(rect.get_x() + rect.get_width()/2, height, label, ha='center', va='bottom', color='red')\n\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Here we see most comments do not have any positive label. These are clean comments. About 4% comments have 1 positive toxicity label and about 6% have 2 or more positive toxicity labels."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Does severe_toxic always mean toxic?\ntrain.loc[(train['severe_toxic'] == 1) & (train['toxic'] != 1), label_names]"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# What are category value looks like when severe_toxic is True?\ntrain[train['severe_toxic'] == 1]"]},{"cell_type":"markdown","metadata":{},"source":["Looks like this is a multi-label classification problem. Based on the value of mean in train data description, most of the categories are labeled 0. We can also see this from count percentage for each label. The count percentages for severe_toxic, identity_hate and threat are below 5%. So, roc_auc might not be a good metric in this case since we are more interested in capturing toxic comments and not so care about clean comments. However, we'll stick to roc_auc since it is a required scoring for this challenge. In practice, we want to consider classification imbalance and other metric such as precision-recall curve might be more appropreate. Also, the severe_toxic is a sub-category of toxic and we see example comments with multiple positive labels."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["import nltk\nnltk.data.path"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import nltk\nnltk.data.path.append(\"/opt/data/share01/jl2408/\")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["\n\n# import stopwords\nfrom nltk.corpus import stopwords\n#from nltk.tokenize import RegexpTokenizer\n\n# function for filter out stopwords\ndef filter_stop_words(sentences, stop_words):\n    filtered = []\n    for sentence in sentences:\n        words = sentence.split()\n        words_filtered = [word for word in words if word not in stop_words]\n        filtered.append(\" \".join(words_filtered))\n    return filtered\n\nstop_words = set(stopwords.words(\"english\"))\n\n# Comments in train\ntrain_cm = train['comment_text']\ntrain_cm_filtered = filter_stop_words(train_cm, stop_words)\n\n# Comments in test_all\ntest_all_cm = test_all['comment_text']\ntest_all_cm_filtered = filter_stop_words(test_all_cm, stop_words)\n\n# Comments in test (excluding labels with -1 values)\ntest_cm = test['comment_text']\ntest_cm_filtered = filter_stop_words(test_cm, stop_words)\n\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# before filter out stop words\ntrain_cm[0]"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# convert to series\ntrain_cm_filtered = pd.Series(train_cm_filtered)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# after filter out stop words\ntrain_cm_filtered[0]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["# Make a copy of the train datasets\nc_train = train.copy(deep=True)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# Add a 'clean' column to the dataframe\nc_train['clean'] = c_train[label_names].sum(axis=1) == 0"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# Number of sentence in a comment\nimport re\nsentence_count = train_cm.apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["# Add 'stopwords_count' column in a comment to dataframe\nc_train['stopwords_count'] = c_train['comment_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["c_train.head()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# Plot number of stopwords in each comment by toxicity or not\nplt.figure(figsize=(12,8))\n\nsns.violinplot(y='stopwords_count',x='clean', data=c_train, split = True)\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of stop words', fontsize=12)\nplt.title(\"Number of stop words in each comment by toxicity or not\", fontsize=15)\n\nplt.show()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# Add 'ip' column in a comment to dataframe\nc_train['ip'] = c_train['comment_text'].apply(lambda x: re.findall(\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\",str(x)))"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# Sample IPs\nc_train['ip'][c_train['ip'].str.len() != 0].head(5)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# Add 'ip_count' column in a comment to dataframe\nc_train['ip_count'] = c_train['ip'].apply(lambda x: len(x))"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# Distribution of ip numbers in a clean comment\nc_train[c_train['clean']]['ip_count'].value_counts()"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# Distribution of ip numbers in a non-clean comment\nc_train[c_train['clean'] == False]['ip_count'].value_counts()"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# Plot and compare number of IPs in clean and non-clean comments\nplt.figure(figsize=(12,8))\n\nsns.violinplot(y='ip_count',x='clean', data=c_train, split = True)\nplt.xlabel('Clean comments?', fontsize=10)\nplt.ylabel('# of IPs', fontsize=10)\nplt.title(\"Compare number of IPs in each comment\", fontsize=15)\n\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["There is no significant indication that a toxic comment contain more IPs. So, we can remove IPs from comments"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# Extract link from comments\nlink = c_train['comment_text'].apply(lambda x: re.findall(\"http://.*com\",str(x)))"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["# Sample links in documents\nlink[link.str.len() != 0].head(5)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# Add 'link_count' column in a comment to dataframe\nc_train['link_count'] = link.apply(lambda x: len(x))"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["# Distribution of link counts\nc_train['link_count'].value_counts()"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# Plot and compare number of links in each comment\nplt.figure(figsize=(12,8))\n\nsns.violinplot(y='link_count',x='clean', data=c_train, split = True)\nplt.xlabel('Clean comments?', fontsize=10)\nplt.ylabel('# of links', fontsize=10)\nplt.title(\"Compare number of links in each comment\", fontsize=15)\n\nplt.show()"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# Extract username from comments\nusername = train_cm.apply(lambda x: re.findall(\"\\[\\[.*\\]\",str(x)))"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# Add 'username_count' column in a comment to dataframe\nc_train['username_count'] = username.apply(lambda x: len(x))"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["# Distribution of username counts\nc_train['username_count'].value_counts()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# Sample of usernames\nusername[username.str.len() != 0].head(5)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# Example comment containing a username\nc_train['comment_text'][140]"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# Plot and compare number of usernames in each comment\nplt.figure(figsize=(12,8))\n\nsns.violinplot(y='username_count',x='clean', data=c_train, split = True)\nplt.xlabel('Clean comments?', fontsize=10)\nplt.ylabel('# of username', fontsize=10)\nplt.title(\"Compare number of usernames in each comment\", fontsize=15)\n\nplt.show()"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["all_cm = pd.concat([train_cm, test_all_cm])"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["# We want to generate features using sklearn tfidfVectorizer from all the comments\n# We limit maximum features to 100000 due to memory constraint\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=100000)\nword_vectorizer.fit(train_cm)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["# Get the feature names\nfeatures = np.array(word_vectorizer.get_feature_names())"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# Generate both train set and test set\ntrain_word_features = word_vectorizer.transform(train_cm)\ntest_all_word_features = word_vectorizer.transform(test_all_cm)\ntest_word_features = word_vectorizer.transform(test_cm)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["train_word_features"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["test_all_word_features"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["test_word_features"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["# https://buhrmann.github.io/tfidf-analysis.html\n# take a single row of the tf-idf matrix (corresponding to a particular document), \n# and return the n highest scoring words (or more generally tokens or features)\ndef top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\n# https://buhrmann.github.io/tfidf-analysis.html\n# convert a single row (row_id) from a sparse matrix (Xtr) into dense format\ndef top_feats_in_doc(Xtr, features, row_id, top_n=25):\n    ''' Top tfidf features in specific document (matrix row) '''\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)"]},{"cell_type":"markdown","metadata":{},"source":["Sample top 10 features on index 1 in train_cm"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["train_cm[1]"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["x = top_feats_in_doc(train_word_features, features, 1, 10)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["print(x)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# What is the vacabulary for the comments\n\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\n\n# for train data\nT = Tokenizer()\nT.fit_on_texts(list(train_cm))\nvocab = len(T.word_index) + 1\nprint('Vocabulary for train comments is {}'.format(vocab))\n\n# for all data\nT = Tokenizer()\nT.fit_on_texts(list(train_cm) + list(test_all_cm))\nvocab = len(T.word_index) + 1\nprint('Vocabulary for all comments is {}'.format(vocab))\n\n# after filter out stopwords\nT = Tokenizer()\nT.fit_on_texts(list(train_cm_filtered) + list(test_all_cm_filtered))\nvocab = len(T.word_index) + 1\nprint('Vocabulary for all comments after filtering out stopwords is {}'.format(vocab))\n\n# for all data exclude labels with -1 values\nT = Tokenizer()\nT.fit_on_texts(list(train_cm) + list(test_cm))\nvocab = len(T.word_index) + 1\nprint('Vocabulary for all comments excluding non-scoring ones is {}'.format(vocab))\n\n# Excluding labels with -1 values and filtered out stopwords\nT = Tokenizer()\nT.fit_on_texts(list(train_cm_filtered) + list(test_cm_filtered))\nvocab = len(T.word_index) + 1\nprint('Vocabulary for all comments after filtering out stopwords and excluding non-scoring ones is {}'.format(vocab))"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["# Check number of comments in train and test dataset\nt_train = T.texts_to_sequences(train_cm_filtered)\nt_test = T.texts_to_sequences(test_cm_filtered)\nprint(len(t_train))\nprint(len(t_test))"]},{"cell_type":"markdown","metadata":{},"source":["Let's look at an example of a text converted to a sequence. Here we see a total of 33 words after removing punctuations and stopwords. word 'I' translated to 2 for example and there are 2 of them. "]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["# A sample comment\ntrain_cm[0]"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["# Words after removing punctuations and stopwords\nword_t = text_to_word_sequence(train_cm_filtered[0])\nprint(len(word_t))\nprint(word_t)"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["# Word sequenced\nprint(len(t_train[0]))\nt_train[0]"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["# Let's find max number of words in a sentence\nn_words_in_a_sentence = [len(x) for x in t_train + t_test]\nprint(max(n_words_in_a_sentence))"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["# Plot number of words in a sentence histgram\nplt.figure(figsize = (8, 6))\nplt.hist(n_words_in_a_sentence,bins = np.arange(0,250,8))\n# Formatting\nplt.xlabel('Number of Words in a Comment Text'); plt.ylabel('Count'); \nplt.title('Fig. 3 Distribution of Number of Words in a Comment Text');\n\nplt.show"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["# Estimate maximum word length to use in word embbedding to cover 97% of the sentences\nn_words_in_a_sentence.sort()\npercent_sentences = 97.3/100.0\na = n_words_in_a_sentence[:int(round(len(n_words_in_a_sentence)*percent_sentences))]\nprint(a[-1])"]},{"cell_type":"markdown","metadata":{},"source":["So, with maximum number of words in a sentence set at 200, we cover over 97 percent of the comments without cut off"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":2}
