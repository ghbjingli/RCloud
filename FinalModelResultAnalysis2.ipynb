{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Machine Learning Engineer Nanodegree\n### Capstone Project\n### Project: Toxic Comment Classification with CNN\n\n### Results Analysis\n\nIn this step, we analyzed the results produced from running the final CNN model with unseen test dataset. The following are performed:\n\n    - calculate the roc_auc score\n    - calculate prediction threshold for tpr > 0.5\n    - calculate prediction threshold for fpr < 0.1\n    - plot roc curves\n    - plot prediction distribution for label 'threat'\n    - generate classification report for prediction threshold corresponding to tpr at 0.5\n    - calculate tpr and fpr for prediction corresponding to tpr at 0.5\n    - calculate Label Ranking Average Precision (LRAP) and Label Ranking Loss (LRL)\n    - calculate roc_auc score for changes in test dataset to see impact of input perturbations\n\nThe default parameters are used for Keras tokenizer:\n\n    - (numwords=None, filters='!\"#$%&()*+,-./:;<=>?@[]^`{|}~ ', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n\nThe following input files are located in the same directory as this notebook.\n\n    1. train.csv - the training set, contains comments with their binary labels\n    2. test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n    3. test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n    4. e25.234.run5.hdf5 - the finale model save in training"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from os import chdir, getcwd\n\nchdir('/opt/data/share01/jl2408/')\ngetcwd() "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Import required classes\nimport tensorflow as tf\nimport keras.backend as K\nimport numpy as np\nimport pandas as pd\nnp.random.seed(8)\nfrom sklearn.metrics import roc_auc_score\nfrom keras.preprocessing import text, sequence\n\n# FROM https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41015\n# AUC for a binary classifier\ndef tf_binary_auc(y_true, y_pred):\n    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n    binSizes = -(pfas[1:]-pfas[:-1])\n    s = ptas*binSizes\n    return K.sum(s, axis=0)\n\n#---------------------\n# PFA, prob false alert for binary classifier\ndef binary_PFA(y_true, y_pred, threshold=K.variable(value=0.8)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # N = total number of negative labels\n    N = K.sum(1 - y_true)\n    # FP = total number of false alerts, alerts from the negative class labels\n    FP = K.sum(y_pred - y_pred * y_true)\n    return FP/N\n\n#----------------\n# P_TA prob true alerts for binary classifier\ndef binary_PTA(y_true, y_pred, threshold=K.variable(value=0.8)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # P = total number of positive labels\n    P = K.sum(y_true)\n    # TP = total number of correct alerts, alerts from the positive class labels\n    TP = K.sum(y_pred * y_true)\n    return TP/P"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Define batch_size and file_path to saved model\nbatch_size=256\nfile_path = 'e25.234.run5.hdf5'\n# Load saved model\nfrom keras.models import load_model\nmodel_saved = load_model(file_path, custom_objects={'tf_binary_auc': tf_binary_auc})\nmodel_saved.summary()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Read in train and test datasets\ntrain = pd.read_csv('train.csv')\ntest_cm = pd.read_csv('test.csv')\ntest_lb = pd.read_csv('test_labels.csv')\ntest_all = pd.merge(test_cm, test_lb, on='id')\ntest = test_all[test_all['toxic'] != -1]\n\n# Create train and test datasets. \n# We need train dataset to recreate tokens to convert test dataset to sequences\nlabel_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nX_train = train[\"comment_text\"]\nX_test = test[\"comment_text\"]\ny_test = test[label_names].values\n\n# Define vocabulary size\nvocab = 100000\n# Define maximum length of a comment\nmaxlen = 200\n\n# Tokenize the train dataset\nt = text.Tokenizer(num_words=vocab)\nt.fit_on_texts(list(X_train))\n\n# Convert test datasets into sequences\nX_test = t.texts_to_sequences(X_test)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Run prediction with saved model on test dataset\ny_pred = model_saved.predict(x_test, batch_size=batch_size)\n# Print out roc_auc score\nscore = roc_auc_score(y_test, y_pred)\nprint(\"No data cleaning and not removing stopwords:\")\nprint(\"\\n roc_auc score: %.6f \\n\" % (score))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Define function plot_roc to compute ROC curves and AUCs for test\ndef plot_roc(y_test, y_pred):\n    from sklearn.metrics import roc_curve, auc\n    from scipy import interp\n\n    # Compute false positive rate (fpr), true positive rate (tpr) and area under the curves (rocauc)\n    fpr = dict()\n    tpr = dict()\n    thr = dict()\n    rocauc = dict()\n    for i in range(y_test.shape[1]):\n        fpr[i], tpr[i], thr[i] = roc_curve(y_test[:, i], y_pred[:, i])\n        rocauc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], thr[\"micro\"] = roc_curve(y_test.ravel(), y_pred.ravel())\n    rocauc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute Macro-average ROC curve and ROC area\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(y_test.shape[1])]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(y_test.shape[1]):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr /= y_test.shape[1]\n\n    # Then interpolate all ROC curves at this points\n    mean_thr = np.zeros_like(all_fpr)\n    for i in range(y_test.shape[1]):\n        mean_thr += interp(all_fpr, fpr[i], thr[i])\n\n    # Finally average it and compute AUC\n    mean_thr /= y_test.shape[1]\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    thr[\"macro\"] = mean_thr\n    rocauc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # to increase tpr, you also increase fpr. It is a trade off between the two.\n    # Business requirement determine which one is more important\n    # We can calculate the threshold to garantee a minimum tpr or a maximum fpr\n    # We see here that to increase tpr, we need to lower threshold\n    # to decrease fpr, we need to increase threshold\n\n    tpr_cut = 0.50\n    fpr_cut = 0.10\n    label = 'threat'\n    idx = label_names.index(label)\n    \n    # index of the first threshold for which the sensitivity > tpr_cut\n    idx1 = np.min(np.where(tpr[\"micro\"] > tpr_cut)) \n    # index of the first threshold for which the fall out < fpr_cut\n    idx2 = np.max(np.where(fpr[\"micro\"] < fpr_cut)) \n    # index of the first threshold for which the sensitivity > tpr_cut\n    idx3 = np.min(np.where(tpr[\"macro\"] > tpr_cut)) \n    # index of the first threshold for which the fall out < fpr_cut\n    idx4 = np.max(np.where(fpr[\"macro\"] < fpr_cut)) \n    # index of the first threshold for which the sensitivity > tpr_cut\n    idx5 = np.min(np.where(tpr[idx] > tpr_cut)) \n    # index of the first threshold for which the fall out < fpr_cut\n    idx6 = np.max(np.where(fpr[idx] < fpr_cut)) \n\n    '''\n    print(\"Micro Average tpr > %.2f: %.4f\" % (tpr_cut,thr[\"micro\"][idx1]))\n    print(\"Micro Average fpr < %.2f: %.4f\" % (fpr_cut,thr[\"micro\"][idx2]))\n    print(\"Macro Average tpr > %.2f: %.4f\" % (tpr_cut,thr[\"macro\"][idx3]))\n    print(\"Macro Average fpr < %.2f: %.4f\" % (fpr_cut,thr[\"macro\"][idx4]))\n    '''\n    print('threshold for label \"{}\" at tpr > {:.2f} is {:.4f}'.format(label,tpr_cut,thr[idx][idx5]))\n    print('threshold for label \"{}\" at fpr < {:.2f} is {:.4f}'.format(label,fpr_cut,thr[idx][idx6]))\n\n    # Plot all ROC curves\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    from itertools import cycle\n\n    plt.figure(figsize=(10,8))\n    lw = 2\n\n    plt.figure(figsize=(10,8))\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.4f})'\n               ''.format(rocauc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.4f})'\n               ''.format(rocauc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n    '''\n    plt.plot([0,fpr[\"micro\"][idx1]], [tpr[\"micro\"][idx1],tpr[\"micro\"][idx1]], 'k--', color='blue')\n    plt.plot([fpr[\"micro\"][idx1],fpr[\"micro\"][idx1]], [0,tpr[\"micro\"][idx1]], 'k--', color='blue')\n\n    plt.plot([0,fpr[\"micro\"][idx2]], [tpr[\"micro\"][idx2],tpr[\"micro\"][idx2]], 'k--', color='red')\n    plt.plot([fpr[\"micro\"][idx2],fpr[\"micro\"][idx2]], [0,tpr[\"micro\"][idx2]], 'k--', color='red')\n\n    plt.plot([0,fpr[\"macro\"][idx3]], [tpr[\"macro\"][idx3],tpr[\"macro\"][idx3]], 'k--', color='blue')\n    plt.plot([fpr[\"macro\"][idx3],fpr[\"macro\"][idx3]], [0,tpr[\"macro\"][idx3]], 'k--', color='blue')\n\n    plt.plot([0,fpr[\"macro\"][idx4]], [tpr[\"macro\"][idx4],tpr[\"macro\"][idx4]], 'k--', color='red')\n    plt.plot([fpr[\"macro\"][idx4],fpr[\"macro\"][idx4]], [0,tpr[\"macro\"][idx4]], 'k--', color='red')\n    '''\n    plt.plot([0,fpr[idx][idx5]], [tpr[idx][idx5],tpr[idx][idx5]], 'k--', color='blue')\n    plt.plot([fpr[idx][idx5],fpr[idx][idx5]], [0,tpr[idx][idx5]], 'k--', color='blue')\n\n    plt.plot([0,fpr[idx][idx6]], [tpr[idx][idx6],tpr[idx][idx6]], 'k--', color='red')\n    plt.plot([fpr[idx][idx6],fpr[idx][idx6]], [0,tpr[idx][idx6]], 'k--', color='red')\n\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(y_test.shape[1]), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of {0} (area = {1:0.4f})'\n             ''.format(label_names[i], rocauc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves and Areas under the Curves')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    return label, idx, thr, idx5, idx6"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Plot the roc curves and auc scores\n# Also print out thresholds for label toxic\nlabel, idx, thr, idx5, idx6 = plot_roc(y_test, y_pred)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Calculate both positive (class=1) and negative (class=0) density\ndf = pd.DataFrame(np.column_stack([y_test, y_pred]))\npos_d=df[df[df.columns[idx]] ==1.0][df.columns[idx + 6]]\nneg_d=df[df[df.columns[idx]] ==0.0][df.columns[idx + 6]]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Plot density distribution\n# Define plot size\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(10,8))\nsns.distplot(pos_d, hist = True, color='green', kde = True, bins=100,\n                 kde_kws = {'linewidth': 3},\n                 label = '1')\nsns.distplot(neg_d, hist = True, color='red', kde = True, bins=100,\n                 kde_kws = {'linewidth': 3},\n                 label = '0')\n\n# Plot formatting\nplt.legend(prop={'size': 16}, title = 'class')\nplt.title('Density with label \"' + label + '\" Prediction Distribution')\nplt.xlabel('Probability')\nplt.ylabel('Density')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Generate classification report corresponding to cutoff at threshold value\nfrom sklearn.metrics import classification_report\nthreshold = thr[idx][idx5]\n#threshold = thr[idx][idx6]\nprint('Classification Report for label \"{}\" at threshold {:.4f}:'.format(label, threshold))\nprint(classification_report(y_test[:,idx], (y_pred[:,idx]>threshold).astype(int)))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Calculate false positive rate corresponding to cutoff at threshold value\nfrom sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(y_test[:,idx], (y_pred[:,idx]>threshold).astype(int)).ravel()\nprint('True Positive Rate for label \"{}\" at threshold {:.4f}:'.format(label, threshold))\nprint(\"tpr: %.4f \\n\" % (1.0*tp/(fn+tp)))\nprint('False Positive Rate for label \"{}\" at threshold {:.4f}:'.format(label, threshold))\nprint(\"fpr: %.4f \\n\" % (1.0*fp/(tn+fp)))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Generate multi label reports\nfrom sklearn.metrics import label_ranking_average_precision_score, label_ranking_loss\n\nprint(\"Label Ranking Average Precision (LRAP): %.6f\" % (label_ranking_average_precision_score(y_test, y_pred)))\nprint(\"Label Ranking Loss (LRL): %.6f\" % (label_ranking_loss(y_test, y_pred)))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Define clean_data function\nimport re\ndef clean_data(test):\n    # Removing ip address\n    X_test_clean = test[\"comment_text\"].apply(lambda x: re.sub(\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\",\"\",x))\n    # Removing url link\n    X_test_clean = X_test_clean.apply(lambda x: re.sub(\"http://.*com\",\"\",x))\n    # Removing username\n    X_test_clean = X_test_clean.apply(lambda x: re.sub(\"\\[\\[.*\\]\",\"\",x))\n    return X_test_clean"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Define sequencing function to convert test datasets into sequences\ndef sequencing(sentence):\n    sentence = t.texts_to_sequences(sentence)\n    x_sentence = sequence.pad_sequences(sentence, maxlen=maxlen)\n    return x_sentence"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import nltk\nnltk.data.path.append('/opt/data/share01/jl2408/')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Filter out stop words\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\ndef filter_stop_words(sentences, stop_words):\n    filtered = []\n    for sentence in sentences:\n        words = sentence.split()\n        words_filtered = [word for word in words if word not in stop_words]\n        filtered.append(\" \".join(words_filtered))\n    return filtered"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Test smal changes in test dataset - with data cleaning and removing stopwords\n# Clean data\nX_test_clean = clean_data(test)\n# Remove stop words\nX_test_clean_no_stopwords = filter_stop_words(X_test_clean, stop_words)\n# Convert test datasets into sequences\nx_test_clean_no_stopwords = sequencing(X_test_clean_no_stopwords)\n# Predict and calculate score\ny_pred_clean_no_stopwords = model_saved.predict(x_test_clean_no_stopwords, batch_size=batch_size)\nscore = roc_auc_score(y_test, y_pred_clean_no_stopwords)\nprint(\"With data cleaning and removing stopwords:\")\nprint(\"roc_auc score: %.6f \\n\" % (score))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Test smal changes in test dataset - data cleaning only\n# Clean data\nX_test_clean = clean_data(test)\n# Remove stop words\n#X_test_clean_no_stopwords = filter_stop_words(X_test_clean, stop_words)\n# Convert test datasets into sequences\nx_test_clean = sequencing(X_test_clean)\n# Predict and calculate score\ny_pred_clean = model_saved.predict(x_test_clean, batch_size=batch_size)\nscore = roc_auc_score(y_test, y_pred_clean)\nprint(\"Data cleaning only:\")\nprint(\"roc_auc score: %.6f \\n\" % (score))"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Test smal changes in test dataset - removing stopwords only\n# Clean data\n#X_test_clean = clean_data(test)\n# Remove stop words\nX_test_no_stopwords = filter_stop_words(test[\"comment_text\"], stop_words)\n# Convert test datasets into sequences\nx_test_no_stopwords = sequencing(X_test_no_stopwords)\n# Predict and calculate score\ny_pred_no_stopwords = model_saved.predict(x_test_no_stopwords, batch_size=batch_size)\nscore = roc_auc_score(y_test, y_pred_no_stopwords)\nprint(\"Removing stopwords only:\")\nprint(\"roc_auc score: %.6f \\n\" % (score))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":2}
