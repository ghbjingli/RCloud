{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Machine Learning Engineer Nanodegree\n### Capstone Project\n### Project: Toxic Comment Classification with CNN\n\n### Solution Implementation\n\nIn this step, we did the following:\n\n    - train the final model selected\n    - run the prediction on the unseen test dataset\n    - plot the roc curves\n    - plot roc_auc score vs epoch\n    - plot loss vs epoch\n    - calculate roc_auc score for changes in test dataset to see impact of input perturbations\n\nThe CNN model has the following parameters:\n\n    - regions: [2,3,4]\n    - number of filters: 32\n    - spatial dropout rate: 0.4\n    - dropout rate: 0.2\n    - activation function: tanh\n    - epochs: 25\n    - batch size: 256\n    - no stop words removing\n    - not removing IPs, usernames and URL links\n    - validation dataset: 10%\n    - optimizer: adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\n    - loss function: binary_crossentropy\n    - metrics: tf_binary_auc, sklearn roc_auc\n\nThe default parameters are used for Keras tokenizer:\n\n    - (numwords=None, filters='!\"#$%&()*+,-./:;<=>?@[]^`{|}~ ', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n\nThe following input files are located in the same directory as this notebook.\n\n    1. train.csv - the training set, contains comments with their binary labels\n    2. test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n    3. test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n    4. crawl-300d-2M.vec - the pre-trained word vectors\n\nOutput files\n\n    1. The best model is saved as hd5f file at the location pointed by file_path variable.\n    2. The prediction results are saved as csv file at the location pointed by save_path variable."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n\n# Import the required classes\nimport numpy as np\nimport pandas as pd\nnp.random.seed(8)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D \nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\n\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in the train and test data\ntrain = pd.read_csv('/opt/data/share01/jl2408/train.csv')\ntest_cm = pd.read_csv('/opt/data/share01/jl2408/test.csv')\ntest_lb = pd.read_csv('/opt/data/share01/jl2408/test_labels.csv')\n# Merge test comments with the labels\ntest_all = pd.merge(test_cm, test_lb, on='id')\n# Create test dataset by removing samples not used for scoring in Kaggle\ntest = test_all[test_all['toxic'] != -1]\n\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Create features and labels for train and test\nlabel_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nX_train = train[\"comment_text\"]\ny_train = train[label_names].values\nX_test = test[\"comment_text\"]\ny_test = test[label_names].values"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Define vocabulary size\nvocab = 100000\n# Define maximum length of a comment\nmaxlen = 200\n# Define embedding size which should equal the embedding size of the pre-trained word vectors\nembed_size = 300"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Tokenize the train dataset\nt = text.Tokenizer(num_words=vocab)\nt.fit_on_texts(list(X_train))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Convert both train and test datasets into sequences\nX_train = t.texts_to_sequences(X_train)\nX_test = t.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Load pre-trained word vectors\nEMBEDDING_FILE = '/opt/data/share01/jl2408/crawl-300d-2M.vec'\nembeddings_index = dict()\nf = open(EMBEDDING_FILE)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s pre-trained words' % len(embeddings_index))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab, embed_size))\nfor word, i in t.word_index.items():\n    if i >= vocab: \n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Import tensorflow class\nimport tensorflow as tf\n# Import keras backend\nimport keras.backend as K\n\n# Define a custom metric to calculate roc_auc based on tensorflow API\n# FROM https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41108\n'''\ndef jacek_auc(y_true, y_pred):\n   score, up_opt = tf.metrics.auc(y_true, y_pred)\n   #score, up_opt = tf.contrib.metrics.streaming_auc(y_pred, y_true)    \n   K.get_session().run(tf.local_variables_initializer())\n   with tf.control_dependencies([up_opt]):\n       score = tf.identity(score)\n   return score\n'''\n\n# Define a custom metric to calculate roc_auc based on tensorflow API\n# FROM https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41015\n# AUC for a binary classifier\ndef tf_binary_auc(y_true, y_pred):\n    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n    binSizes = -(pfas[1:]-pfas[:-1])\n    s = ptas*binSizes\n    return K.sum(s, axis=0)\n\n#---------------------\n# PFA, prob false alert for binary classifier\ndef binary_PFA(y_true, y_pred, threshold=K.variable(value=0.8)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # N = total number of negative labels\n    N = K.sum(1 - y_true)\n    # FP = total number of false alerts, alerts from the negative class labels\n    FP = K.sum(y_pred - y_pred * y_true)\n    return FP/N\n\n#----------------\n# P_TA prob true alerts for binary classifier\ndef binary_PTA(y_true, y_pred, threshold=K.variable(value=0.8)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # P = total number of positive labels\n    P = K.sum(y_true)\n    # TP = total number of correct alerts, alerts from the positive class labels\n    TP = K.sum(y_pred * y_true)\n    return TP/P"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n\n# Create a class to calculate Sklearn roc_auc_score for both train and validation\n# at the end of batch and epoch\nfrom sklearn.metrics import roc_auc_score\nimport keras\nclass RocAucMetricCallback(Callback):\n    def __init__(self, train_data, predict_batch_size=1024, include_on_batch=False):\n        super(RocAucMetricCallback, self).__init__()\n        self.train_data=train_data\n        self.predict_batch_size=predict_batch_size\n        self.include_on_batch=include_on_batch\n\n    def on_batch_begin(self, batch, logs={}):\n        pass\n\n    def on_batch_end(self, batch, logs={}):\n        if(self.include_on_batch):\n            logs['sklearn_auc']=float('-inf')\n            logs['val_sklearn_auc']=float('-inf')\n            if(self.train_data):\n                logs['sklearn_auc']=roc_auc_score(self.train_data[1], \n                                                  self.model.predict(self.train_data[0],\n                                                                     batch_size=self.predict_batch_size))\n            if(self.validation_data):\n                logs['val_sklearn_auc']=roc_auc_score(self.validation_data[1], \n                                                  self.model.predict(self.validation_data[0],\n                                                                     batch_size=self.predict_batch_size))\n\n    def on_train_begin(self, logs={}):\n        if not ('sklearn_auc' in self.params['metrics']):\n            self.params['metrics'].append('sklearn_auc')\n        if not ('val_sklearn_auc' in self.params['metrics']):\n            self.params['metrics'].append('val_sklearn_auc')\n\n    def on_train_end(self, logs={}):\n        pass\n\n    def on_epoch_begin(self, epoch, logs={}):\n        pass\n\n    def on_epoch_end(self, epoch, logs={}):\n        logs['sklearn_auc']=float('-inf')\n        logs['val_sklearn_auc']=float('-inf')\n        if(self.train_data):\n            logs['sklearn_auc']=roc_auc_score(self.train_data[1], \n                                              self.model.predict(self.train_data[0],\n                                                                 batch_size=self.predict_batch_size))\n            print(\"- sklearn_auc: %.4f\" % (logs['sklearn_auc']))    \n        if(self.validation_data):\n            logs['val_sklearn_auc']=roc_auc_score(self.validation_data[1], \n                                              self.model.predict(self.validation_data[0],\n                                                                 batch_size=self.predict_batch_size))\n            print(\"- val_sklearn_auc: %.4f\" % (logs['val_sklearn_auc']))    \n\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Define make_model function to create a CNN model\ndef make_model(k=[2,3,4], activation='tanh', filters=32, Sdroprate=0.4, droprate=0.2):\n\n    # Input layer\n    inp = Input(shape=(maxlen, ))\n    # embedding layer\n    x = Embedding(vocab, embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False)(inp)\n    # Spatial dropout layer\n    x = SpatialDropout1D(Sdroprate)(x)\n    # Reshape to connect to 2D layer\n    x = Reshape((maxlen, embed_size, 1))(x)\n    \n    # Convolutional and 1-max pooling layers\n    conv = dict()\n    maxpool = dict()\n    for h in k:\n        conv[h] = Conv2D(filters, kernel_size=(h, embed_size), activation=activation)(x)\n        maxpool[h] = MaxPool2D(pool_size=(maxlen - h + 1, 1))(conv[h])\n    if len(k) == 1:\n        y = maxpool[h]\n    else:\n        # Concatenate multiple 1-max pools\n        y = Concatenate(axis=1)([pool for key,pool in maxpool.items()])\n    # Flatten layer\n    y = Flatten()(y)\n    # Dropout layer\n    y = Dropout(droprate)(y)\n    # Output dense layer\n    outp = Dense(6, activation=\"sigmoid\")(y)\n    # Generate the cnn model\n    model = Model(inputs=inp, outputs=outp)\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=[tf_binary_auc])\n    \n    return model\n# Make the model\nmodel = make_model()\n# Print out model summary\nmodel.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n\n# Split the train dataset into train and validation datasets\nx_trainS, x_val, y_trainS, y_val = train_test_split(x_train, y_train, train_size=0.90, random_state=8)\n\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Set batch_size and epochs parameter\nbatch_size=256\nepochs = 25\n# Define file_path to store best model\nfile_path = '/opt/data/share01/jl2408/e25.234.run5.hdf5'\n# Create instance of ModelCheckpoint, EarlyStopping and RocAucMetricCallback\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\ncheck_point = ModelCheckpoint(filepath=file_path, monitor = 'val_sklearn_auc', mode=\"max\", verbose=1, save_best_only=True)\nearly_stop = EarlyStopping(monitor = 'val_sklearn_auc', mode = \"max\", patience = 2, verbose=2)\nroc_auc = RocAucMetricCallback(train_data=(x_trainS, y_trainS))\n# Define callback\n#cb = [roc_auc, early_stop, check_point]\ncb = [roc_auc, check_point]\n# Fit the model with the train and validation datasets\nfrom keras.wrappers.scikit_learn import KerasClassifier\nestimator = KerasClassifier(make_model,epochs=epochs,batch_size=batch_size,verbose=1)\nh = estimator.fit(x_trainS, y_trainS, validation_data=(x_val, y_val), callbacks=cb)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Load the saved best model and print out the model summary\nfrom keras.models import load_model\nmodel_saved = load_model(file_path, custom_objects={'tf_binary_auc': tf_binary_auc})\nmodel_saved.summary()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Run prediction with the saved model on unseen test dataset and calculate the roc_auc score\ny_pred = model_saved.predict(x_test, batch_size=batch_size)\nscore = roc_auc_score(y_test, y_pred)\nprint(\"\\n roc_auc score: %.6f \\n\" % (score))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Compute ROC curves and AUCs for test\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\n# Compute false positive rate (fpr), true positive rate (tpr) and area under the curves (rocauc)\nfpr = dict()\ntpr = dict()\nthr = dict()\nrocauc = dict()\nfor i in range(y_test.shape[1]):\n    fpr[i], tpr[i], thr[i] = roc_curve(y_test[:, i], y_pred[:, i])\n    rocauc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], thr[\"micro\"] = roc_curve(y_test.ravel(), y_pred.ravel())\nrocauc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Compute Macro-average ROC curve and ROC area\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(y_test.shape[1])]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(y_test.shape[1]):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= y_test.shape[1]\n\n# Then interpolate all ROC curves at this points\nmean_thr = np.zeros_like(all_fpr)\nfor i in range(y_test.shape[1]):\n    mean_thr += interp(all_fpr, fpr[i], thr[i])\n\n# Finally average it and compute AUC\nmean_thr /= y_test.shape[1]\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nthr[\"macro\"] = mean_thr\nrocauc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# to increase tpr, you also increase fpr. It is a trade off between the two.\n# Business requirement determine which one is more important\n# We can calculate the threshold to garantee a minimum tpr or a maximum fpr\n# We see here that to increase tpr, we need to lower threshold\n# to decrease fpr, we need to increase threshold\n\ntpr_cut = 0.90\nfpr_cut = 0.10\n\n# index of the first threshold for which the sensitivity > tpr_cut\nidx1 = np.min(np.where(tpr[\"micro\"] > tpr_cut)) \n# index of the first threshold for which the fall out < fpr_cut\nidx2 = np.max(np.where(fpr[\"micro\"] < fpr_cut)) \n# index of the first threshold for which the sensitivity > tpr_cut\nidx3 = np.min(np.where(tpr[\"macro\"] > tpr_cut)) \n# index of the first threshold for which the fall out < fpr_cut\nidx4 = np.max(np.where(fpr[\"macro\"] < fpr_cut)) \n# index of the first threshold for which the sensitivity > tpr_cut\nidx5 = np.min(np.where(tpr[0] > tpr_cut)) \n# index of the first threshold for which the fall out < fpr_cut\nidx6 = np.max(np.where(fpr[0] < fpr_cut)) \n\n'''\nprint(\"Micro Average tpr > %.2f: %.4f\" % (tpr_cut,thr[\"micro\"][idx1]))\nprint(\"Micro Average fpr < %.2f: %.4f\" % (fpr_cut,thr[\"micro\"][idx2]))\nprint(\"Macro Average tpr > %.2f: %.4f\" % (tpr_cut,thr[\"macro\"][idx3]))\nprint(\"Macro Average fpr < %.2f: %.4f\" % (fpr_cut,thr[\"macro\"][idx4]))\n'''\nprint(\"toxic label tpr > %.2f: %.4f\" % (tpr_cut,thr[0][idx5]))\nprint(\"toxic label fpr < %.2f: %.4f\" % (fpr_cut,thr[0][idx6]))\n\n# Plot all ROC curves\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import cycle\n\nplt.figure(figsize=(10,8))\nlw = 2\n\nplt.figure(figsize=(10,8))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.4f})'\n               ''.format(rocauc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.4f})'\n               ''.format(rocauc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n'''\nplt.plot([0,fpr[\"micro\"][idx1]], [tpr[\"micro\"][idx1],tpr[\"micro\"][idx1]], 'k--', color='blue')\nplt.plot([fpr[\"micro\"][idx1],fpr[\"micro\"][idx1]], [0,tpr[\"micro\"][idx1]], 'k--', color='blue')\n\nplt.plot([0,fpr[\"micro\"][idx2]], [tpr[\"micro\"][idx2],tpr[\"micro\"][idx2]], 'k--', color='red')\nplt.plot([fpr[\"micro\"][idx2],fpr[\"micro\"][idx2]], [0,tpr[\"micro\"][idx2]], 'k--', color='red')\n\nplt.plot([0,fpr[\"macro\"][idx3]], [tpr[\"macro\"][idx3],tpr[\"macro\"][idx3]], 'k--', color='blue')\nplt.plot([fpr[\"macro\"][idx3],fpr[\"macro\"][idx3]], [0,tpr[\"macro\"][idx3]], 'k--', color='blue')\n\nplt.plot([0,fpr[\"macro\"][idx4]], [tpr[\"macro\"][idx4],tpr[\"macro\"][idx4]], 'k--', color='red')\nplt.plot([fpr[\"macro\"][idx4],fpr[\"macro\"][idx4]], [0,tpr[\"macro\"][idx4]], 'k--', color='red')\n'''\nplt.plot([0,fpr[0][idx5]], [tpr[0][idx5],tpr[0][idx5]], 'k--', color='blue')\nplt.plot([fpr[0][idx5],fpr[0][idx5]], [0,tpr[0][idx5]], 'k--', color='blue')\n\nplt.plot([0,fpr[0][idx6]], [tpr[0][idx6],tpr[0][idx6]], 'k--', color='red')\nplt.plot([fpr[0][idx6],fpr[0][idx6]], [0,tpr[0][idx6]], 'k--', color='red')\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(y_test.shape[1]), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of {0} (area = {1:0.4f})'\n             ''.format(label_names[i], rocauc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves and Areas under the Curves')\nplt.legend(loc=\"lower right\")\nplt.show()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Visualize training history\nimport matplotlib.pyplot as plt\nimport numpy\n\n# list all data in history\nprint(h.history.keys())"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["\n\n# Plot history for roc_auc scores\nplt.figure(figsize=(10,8))\n#plt.plot(h.history['tf_binary_auc'])\n#plt.plot(h.history['val_tf_binary_auc'])\nplt.plot(h.history['sklearn_auc'])\nplt.plot(h.history['val_sklearn_auc'])\nplt.title('model roc_auc vs epoch')\nplt.ylabel('auc')\nplt.xlabel('epoch')\n#plt.legend(['tf binary auc train', 'tf binary auc test', 'sklearn auc train', 'sklearn auc test'], loc='upper left')\nplt.legend(['sklearn auc train', 'sklearn auc validation'], loc='upper left')\nplt.show()\n# Plot history for loss\nplt.figure(figsize=(10,8))\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('model loss vs epoch')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Save the prediction on test dataset\nsave_path = '/opt/data/share01/jl2408/pred_run5-234-tanh-25e-sd0.4-d0.2-adadelta-dot9830.csv'\npred_save = test.copy()\npred_save[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\npred_save.to_csv(save_path, index=False)\npred_test = pd.read_csv(save_path)\npred_test = pred_test[label_names]\nscore = roc_auc_score(y_test, pred_test)\nprint(\"\\n roc_auc score: %.6f \\n\" % (score))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":2}
